{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By chat model we mean LLM model which operates with chats \n",
    "import os \n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI \n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal \n",
    "\n",
    "## check performance in agent workflow \n",
    "from typing import TypedDict, Annotated, List, Dict, Optional\n",
    "from langchain_core.messages import BaseMessage, AnyMessage, ToolMessage,HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import add_messages , START, END , StateGraph\n",
    "from IPython.display import Image, display \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# prepare model for embeddings  \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "import numpy.typing as npt\n",
    "\n",
    "from tavily import TavilyClient \n",
    "\n",
    "import subprocess\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "## OpenAI configuration\n",
    "api_key_var = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#print(\"OpenAI API: \" , api_key_var)\n",
    "\n",
    "\n",
    "# keys for tavily \n",
    "tavily_api = os.environ.get(\"TAVILY_API_KEY\")\n",
    "#print(\"Tavily API: \" , tavily_api)\n",
    "\n",
    "# api for goole\n",
    "api_google = os.environ.get('GOOGLE_API_KEY')\n",
    "#print(\"Google API: \" , api_google)\n",
    "\n",
    "# define search engine\n",
    "SEARCH_ENGINE_ID = '26dda816634bd4044'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc22b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test OpenAI api\n",
    "# model as basis for tool payload creation \n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",           # BEST: Most reliable for tool calling\n",
    "                              # Better function calling accuracy than gpt-4o-mini\n",
    "                              # Handles complex tool sequences properly\n",
    "    \n",
    "    temperature=0,            # PERFECT: Maximum precision for tool calls\n",
    "                              # Deterministic parameter selection\n",
    "                              # Consistent function calling behavior\n",
    "    \n",
    "    max_retries=3,            # ESSENTIAL: Tool calls can fail, need retries\n",
    "                              # Network issues more critical with function calling\n",
    "    \n",
    "    timeout=30,               # ADEQUATE: Tool calls need processing time\n",
    "                              # Allows for complex parameter reasoning\n",
    "    \n",
    "    api_key=api_key_var\n",
    ")\n",
    "\n",
    "\n",
    "#llm.invoke(input=\"Hello\") # get AI message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding model\n",
    "embedding_main = OpenAIEmbeddings(api_key=api_key_var, model=\"text-embedding-3-small\")\n",
    "embedding_cross_lang = OpenAIEmbeddings(api_key=api_key_var, model=\"text-embedding-3-large\")\n",
    "\n",
    "# For HyDE comparison we will employ large embedding model for richer semantic representation\n",
    "# Larger model should be able to capture same concepts in different languages \n",
    "# Similar concepts should be positioned closer in the vector space\n",
    "\n",
    "# Test with similar concepts + unrelated text\n",
    "test_sentences = [\n",
    "    \"Искусственный интеллект меняет мир\",        # AI is changing the world (Russian)\n",
    "    \"Artificial intelligence is changing the world\",  # AI is changing the world (English) - fixed typo \"word\" -> \"world\"\n",
    "    \"Я люблю есть пиццу по вечерам\"              # I love eating pizza in the evenings (unrelated)\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = [np.array(embedding_cross_lang.embed_query(x)).reshape(1, -1) for x in test_sentences]\n",
    "\n",
    "# Calculate similarities\n",
    "print(\"Similarity between Russian and English (should be HIGH):\")\n",
    "print(f\"  {cosine_similarity(embeddings[0], embeddings[1])[0][0]:.4f}\")\n",
    "\n",
    "print(\"\\nSimilarity between Russian AI and Pizza (should be LOW):\")\n",
    "print(f\"  {cosine_similarity(embeddings[0], embeddings[2])[0][0]:.4f}\")\n",
    "\n",
    "print(\"\\nSimilarity between English AI and Pizza (should be LOW):\")\n",
    "print(f\"  {cosine_similarity(embeddings[1], embeddings[2])[0][0]:.4f}\")\n",
    "\n",
    "del embeddings\n",
    "\n",
    "# large vs small:\n",
    "# better at capturing same meaning in different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to python 4_4_5_research_assistant.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481b07e",
   "metadata": {},
   "source": [
    "#### LLM models api definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation and key terms extraction from short input\n",
    "llm_translation_or_terms = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # BEST: Cheapest model that excels at translation tasks\n",
    "                          # Translation is pattern-matching, doesn't need reasoning power\n",
    "                          # GPT-4o-mini handles languages perfectly at 10x lower cost\n",
    "    \n",
    "    temperature=0.1,      # BEST: Near-deterministic output for consistent translations\n",
    "                          # Translation should be consistent, not creative\n",
    "                          # 0.1 allows tiny variation while preventing hallucinations\n",
    "    \n",
    "    max_tokens=100,       # BEST: Perfect for short translations and key terms\n",
    "                          # Prevents model from over-explaining or adding fluff\n",
    "                          # Saves money by limiting output length\n",
    "    \n",
    "    timeout=5,           # BEST: Translation should be fast, 5s is generous\n",
    "                          # If it takes longer, something's wrong with the request\n",
    "                          # Prevents hanging requests from eating budget\n",
    "    \n",
    "    max_retries=2         # BEST: Translation usually works first try\n",
    "                          # 2 retries handles temporary network issues\n",
    "                          # More retries waste time and money on bad requests\n",
    ")\n",
    "\n",
    "# Summary of content extracted from URL with tavily  \n",
    "llm_url_content = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # BETTER CHOICE: Your original \"gpt-4o\" was budget-killer\n",
    "                          # URL content analysis is mostly extraction/summarization\n",
    "                          # GPT-4o-mini handles this 90% as well for 10% of the cost\n",
    "                          # Save premium models for truly complex reasoning\n",
    "    \n",
    "    temperature=0.2,      # BEST: Factual analysis needs consistency, not creativity\n",
    "                          # Prevents model from \"making up\" facts about content\n",
    "                          # Ensures similar content gets similar analysis\n",
    "    \n",
    "    max_tokens=1500,      # GOOD: Enough for detailed analysis without waste\n",
    "                          # Your original 1500 was excessive for most URL content\n",
    "                          # Can capture key points without paying for verbosity\n",
    "    \n",
    "    top_p=0.95,          # GOOD: Slight randomness reduction for factual tasks\n",
    "                          # Works with low temperature to ensure accuracy\n",
    "                          # 0.95 allows some vocabulary variation while staying factual\n",
    "    \n",
    "    timeout=15,          # BETTER: Reduced from implicit 60s default\n",
    "                          # Content analysis shouldn't take forever\n",
    "                          # Faster feedback loop for debugging\n",
    ")\n",
    "\n",
    "\n",
    "# # generate persona\n",
    "llm_class_generation = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",     # BETTER: Cheaper, perfectly capable for structured output\n",
    "                             # Class generation is simple task, save budget\n",
    "    \n",
    "    temperature=0.2,         # BEST: Near-deterministic for consistent structure\n",
    "                             # You want predictable field values\n",
    "    \n",
    "    max_tokens=600,          # CORRECT: 3 fields with descriptions = ~50-80 tokens\n",
    "                             # Prevents rambling, forces concise responses\n",
    "    \n",
    "    timeout=60,             # SUFFICIENT: Simple generation shouldn't take long\n",
    "    \n",
    "    max_retries=2           # ADEQUATE: Structured output usually works first try\n",
    ")\n",
    "# LengthFinishReasonError: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=200, prompt_tokens=426, total_\n",
    "# APITimeoutError: Request timed out.\n",
    "\n",
    "\n",
    "llm_hyde_generation = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",     # OPTIMAL: Cost-effective for creative writing\n",
    "                             # Hyde articles don't need complex reasoning\n",
    "                             # Good at generating diverse writing styles\n",
    "    \n",
    "    temperature=0.5,         # BEST: Creative variation for different journalist styles\n",
    "                             # Each journalist should write distinctively\n",
    "                             # Higher temp = more stylistic diversity\n",
    "    \n",
    "    max_tokens=500,          # CORRECT: 200-300 word articles need ~350-400 tokens\n",
    "                             # Allows for complete thoughts and proper structure\n",
    "                             # Prevents truncation mid-sentence\n",
    "    \n",
    "    timeout=60,             # ADEQUATE: Article generation needs more time than classes\n",
    "                             # Creative writing takes longer than structured output\n",
    "    \n",
    "    max_retries=2           # SUFFICIENT: Article generation usually succeeds first try\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Final summarization of multiple content pieces\n",
    "llm_summarization = ChatOpenAI(\n",
    "    model=\"gpt-4o\",       # CORRECT: You need the reasoning power here!\n",
    "                          # Multi-document synthesis requires intelligence\n",
    "                          # Finding connections across articles needs advanced reasoning\n",
    "                          # 128K context window handles multiple long articles\n",
    "                          # This is where you should spend your budget\n",
    "    \n",
    "    temperature=0.1,      # BEST: Even complex analysis should be consistent\n",
    "                          # You want reliable insights, not creative interpretations\n",
    "    \n",
    "    max_tokens=3000,      # CORRECT: Complex multi-article analysis needs space\n",
    "                          # Executive summaries of multiple sources require detail\n",
    "                          # You need room for nuanced insights and connections\n",
    "    \n",
    "    top_p=0.95,          # GOOD: Allows sophisticated language while staying accurate\n",
    "    \n",
    "    timeout=45,          # INCREASED: Complex reasoning takes time\n",
    "                          # Multi-document analysis can't be rushed\n",
    "                          # 45s allows for thoughtful processing\n",
    "    \n",
    "    max_retries=3        # CORRECT: Complex tasks may need retries\n",
    "                          # Network issues more likely with longer processing\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cede84",
   "metadata": {},
   "source": [
    "#### Main classes for workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe052d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Support for journalists and HyDe articles \n",
    "class Journalist(BaseModel):\n",
    "    expertise: str = Field(\n",
    "        description=\"Primary area of expertise and writing focus.\"\n",
    "    )\n",
    "    perspective: str = Field(\n",
    "        description=\"Writing perspective and approach to the topic.\"\n",
    "    )\n",
    "    style: str = Field(\n",
    "        description=\"Writing style, tone, and target audience.\"\n",
    "    )\n",
    "\n",
    "class HydePerspectives(BaseModel):\n",
    "      journalists: List[ Journalist ] = Field( \"Comprehensive list of analysts with their roles and affiliations.\" )\n",
    "\n",
    "## test generation of personalitis \n",
    "\n",
    "structured_llm = llm_class_generation.with_structured_output(schema=HydePerspectives)     \n",
    "response = structured_llm.invoke( [HumanMessage(content=\"Populate the class with 2 writer\")] )\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evidence Lineage tracking \n",
    "AllowedClaimType = Literal[\"fine\", \"investigation\", \"allegation\", \"clearance\", \"settlement\"]\n",
    "\n",
    "class EvidenceClaim(BaseModel):\n",
    "    \"\"\"Track claims extracted from summaries - focused on consolidation\"\"\"\n",
    "    claim_text: str = Field(description=\"Specific claim made (e.g., 'FinCEN fined company €50M in 2023')\")\n",
    "    claim_type: AllowedClaimType = Field(description=\"Type: fine|investigation|allegation|clearance|settlement\")\n",
    "    supporting_urls: List[str] = Field(description=\"ALL URLs that mention this claim\")\n",
    "    date_publish: str = Field(description=\" Publication date or event date \")\n",
    "\n",
    "\n",
    "class ClaimsFromSummaries( BaseModel ): \n",
    "    evidence_claims: List[EvidenceClaim] = Field(\n",
    "    description=\"All extractable claims from the summaries\"\n",
    "      )\n",
    "\n",
    "# Accountability: Every claim is traced to specific source\n",
    "# Credibility: Check if the claim is supported by evidence\n",
    "# Gap identification: We know what information is missing from the data\n",
    "# Iteration: Check how many times we researched missing evidences \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support of google search arguments \n",
    "class candidate_names(BaseModel):\n",
    "       comp_names_variation: List[str] = Field(description=\"Different possible variations of company name\")\n",
    "\n",
    "class search_terms_extraction(BaseModel):\n",
    "       or_terms: List[str] = Field(description=\"List of alternative terms and variations for OR search in the specified language\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data and Meta for extracted content\n",
    "class LinkCollection(BaseModel):\n",
    "       displayLink: str = Field(description=\"The display URL shown in search results (usually domain name)\")\n",
    "       link: str = Field(description=\"The full URL of the search result\")\n",
    "       raw_content: str = Field(default=\"\", description=\"Content extracted from URL\")\n",
    "       summary: str = Field(default=\"\", description=\"Summary of content extracted from URL\")\n",
    "       hyde_score: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Max value of similarity between content of web page and HyDe articles\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main state\n",
    "class UnifiedResearchState(TypedDict):\n",
    "    # From UnifiedResearchState\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    search_results: List[LinkCollection]\n",
    "    filtered_results: List[LinkCollection]\n",
    "    entity_name: str\n",
    "    expanded_query:str\n",
    "    \n",
    "    # From GenerateAnalystsState\n",
    "    max_journalists: int\n",
    "    journalists: List[Journalist]\n",
    "    hyde_list: List[str]    \n",
    "\n",
    "    # from evidence collection \n",
    "    evidence_claims: List[EvidenceClaim]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95aa02a",
   "metadata": {},
   "source": [
    "#### Supportive tools definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate different names of the company\n",
    "# can be used later\n",
    "\n",
    "entity_name = \"Lukoil\"\n",
    "\n",
    "\n",
    "def extract_name_variations(company_name: str, llm) -> candidate_names:\n",
    "    \"\"\"Generate different variations of how a company name might appear in news articles\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"company_name\"],\n",
    "        template='''\n",
    "You are a text analysis expert. Given the original company name: \"{company_name}\"\n",
    "\n",
    "Generate 4-6 possible ways this company name could appear in news articles, press releases, and public documents.\n",
    "\n",
    "Consider these common variations:\n",
    "1. Full official name with legal entity type (PJSC, LLC, Inc., Corp., Ltd.)\n",
    "2. Shortened version without legal entity type\n",
    "3. Common abbreviations or acronyms\n",
    "4. Alternative spellings or transliterations\n",
    "5. How it might appear in headlines (often shortened)\n",
    "6. Brand name vs. legal entity name\n",
    "7. Regional variations or subsidiaries\n",
    "\n",
    "For example:\n",
    "- Original: \"PJSC Lukoil Oil Company\"\n",
    "- Variations: [\"PJSC Lukoil\", \"Lukoil Oil Company\", \"Lukoil\", \"Lukoil Oil\"]\n",
    "\n",
    "- Original: \"Apple Inc.\"\n",
    "- Variations: [\"Apple Inc\", \"Apple\", \"Apple Computer\", \"AAPL\", \"Apple Corporation\"]\n",
    "\n",
    "Original Company Name: {company_name}\n",
    "\n",
    "Generate ONLY the list of name variations as a JSON array of strings.\n",
    "        '''\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(candidate_names)\n",
    "    chain = prompt | structured_llm\n",
    "    response = chain.invoke({\"company_name\": company_name})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Usage:\n",
    "result = extract_name_variations(\"PJSC Lukoil Oil Company\", llm)\n",
    "variations_list = result.comp_names_variation\n",
    "variations_list.append(entity_name)\n",
    "variations_list\n",
    "\" \".join( variations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define argument for google search \n",
    "\n",
    "def extract_orterms_from_query(search_query: str, llm, ln: str) -> str:\n",
    "    \"\"\"Extract relevant terms from search query for Google orTerms parameter in the specified language\n",
    "    \n",
    "    Args:\n",
    "        search_query (str): The original search query\n",
    "        llm: Language model instance\n",
    "        ln (str): Language code ('en', 'ru', 'fr', 'ro', 'de')\n",
    "        \n",
    "    Returns:\n",
    "        str: Space-separated string of OR terms in the specified language\n",
    "    \"\"\"\n",
    "    \n",
    "    # Language code to full name mapping\n",
    "    language_mapping = {\n",
    "        'en': 'English',\n",
    "        'ru': 'Russian',\n",
    "        'fr': 'French', \n",
    "        'ro': 'Romanian',\n",
    "        'de': 'German'\n",
    "    }\n",
    "    \n",
    "    # Get full language name or default to English\n",
    "    full_language_name = language_mapping.get(ln.lower(), 'English')\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"search_query\", \"language\"],\n",
    "        template='''\n",
    "You are a search optimization expert. Analyze this search query and extract alternative terms that should be used in Google's orTerms parameter.\n",
    "\n",
    "IMPORTANT: Generate ALL terms in {language} language. The response must be entirely in the specified language.\n",
    "\n",
    "Original Query: \"{search_query}\"\n",
    "\n",
    "Extract 6-10 alternative terms covering:\n",
    "1. Company name variations (abbreviations, alternative spellings, transliterations), both in english and  {language}\n",
    "2. Topic/subject variations (synonyms, related terms, technical terms)\n",
    "3. Activity variations (different ways to describe the same concept)\n",
    "\n",
    "For money laundering searches, consider terms like:\n",
    "- Financial crimes, compliance violations, sanctions evasion\n",
    "- AML violations, illicit finance, financial misconduct\n",
    "- Regulatory violations, corruption, fraud\n",
    "\n",
    "For company names, consider:\n",
    "- Official names with/without legal entity types\n",
    "- Common abbreviations, brand names\n",
    "- Alternative spellings or transliterations\n",
    "\n",
    "IMPORTANT: Keep each term SHORT - single words or very short phrases work best for OR logic.\n",
    "CRITICAL: All terms must be in {language} language.\n",
    "\n",
    "Example for English:\n",
    "Query: \"Search for news about Apple Inc related to tax evasion\"\n",
    "orTerms: [\"Apple\", \"AAPL\", \"Apple-Inc\", \"tax-evasion\", \"tax-avoidance\", \"taxation\", \"fiscal\", \"treasury\", \"IRS\", \"revenue\"]\n",
    "\n",
    "Example for Romanian:\n",
    "Query: \"Căutați știri despre Apple Inc legate de evaziunea fiscală\"\n",
    "orTerms: [\"Apple\", \"AAPL\", \"Apple-Inc\", \"evaziune-fiscală\", \"fraudă-fiscală\", \"taxare\", \"fiscal\", \"trezorerie\", \"venituri\", \"impozite\"]\n",
    "\n",
    "Example for Russian:\n",
    "Query: \"Поиск новостей об Apple Inc связанных с уклонением от налогов\"\n",
    "orTerms: [\"Apple\", \"AAPL\", \"Apple-Inc\", \"уклонение-налогов\", \"налоговое-мошенничество\", \"налогообложение\", \"фискальный\", \"казначейство\", \"доходы\", \"налоги\"]\n",
    "\n",
    "Example for French:\n",
    "Query: \"Rechercher des nouvelles sur Apple Inc liées à l'évasion fiscale\"\n",
    "orTerms: [\"Apple\", \"AAPL\", \"Apple-Inc\", \"évasion-fiscale\", \"fraude-fiscale\", \"taxation\", \"fiscal\", \"trésorerie\", \"revenus\", \"impôts\"]\n",
    "\n",
    "Example for German:\n",
    "Query: \"Suchen Sie nach Nachrichten über Apple Inc bezüglich Steuerhinterziehung\"\n",
    "orTerms: [\"Apple\", \"AAPL\", \"Apple-Inc\", \"Steuerhinterziehung\", \"Steuerumgehung\", \"Steuervermeidung\", \"Steuerdelikte\", \"Steuerrecht\", \"Finanzbehörden\", \"Steuerprüfung\", \"Umsätze\", \"Gewinne\", \"Steuern\"]\n",
    "\n",
    "Query: \"{search_query}\"\n",
    "Language: {language}\n",
    "\n",
    "Generate ONLY a list of SHORT alternative terms as a JSON array of strings in {language} language.\n",
    "        '''\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(search_terms_extraction)\n",
    "    chain = prompt | structured_llm\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"search_query\": search_query, \n",
    "        \"language\": full_language_name\n",
    "    })\n",
    "    \n",
    "    # Join the terms with spaces for orTerms parameter\n",
    "    orterms_string = \" \".join(response.or_terms)\n",
    "    \n",
    "    return orterms_string\n",
    "\n",
    "# Usage examples:\n",
    "# English\n",
    "search_query = \"Search for news and information about TESLA Company company related to money laundering.\"\n",
    "orterms_for_search = extract_orterms_from_query(search_query, llm_translation_or_terms, \"en\")\n",
    "\n",
    "print(f\"Original query: {search_query}\")\n",
    "print(f\"Generated orTerms: {orterms_for_search}\")\n",
    "\n",
    "# Romanian\n",
    "search_query = \"Căutați știri și informații despre compania PJSC Lukoil Oil Company legate de spălarea banilor.\"\n",
    "orterms_for_search = extract_orterms_from_query(search_query, llm_translation_or_terms, \"ro\")\n",
    "\n",
    "print(f\"Original query: {search_query}\")\n",
    "print(f\"Generated orTerms: {orterms_for_search}\")\n",
    "\n",
    "# Russian\n",
    "#search_query = \"Поиск новостей и информации о компании PJSC Lukoil Oil Company связанных с отмыванием денег.\"\n",
    "#orterms_for_search = extract_orterms_from_query(search_query, llm_translation_or_terms, \"ru\")\n",
    "\n",
    "#print(f\"Original query: {search_query}\")\n",
    "#print(f\"Generated orTerms: {orterms_for_search}\")\n",
    "\n",
    "# French\n",
    "#search_query = \"Rechercher des nouvelles et informations sur la société PJSC Lukoil Oil Company liées au blanchiment d'argent.\"\n",
    "#orterms_for_search = extract_orterms_from_query(search_query, llm_translation_or_terms, \"fr\")\n",
    "\n",
    "#print(f\"Original query: {search_query}\")\n",
    "#print(f\"Generated orTerms: {orterms_for_search}\")\n",
    "\n",
    "# German\n",
    "#search_query = \"Suchen Sie nach Nachrichten und Informationen über die Gesellschaft PJSC Lukoil Oil Company im Zusammenhang mit Geldwäsche.\"\n",
    "#orterms_for_search = extract_orterms_from_query(search_query, llm_translation_or_terms, \"fr\")\n",
    "\n",
    "#print(f\"Original query: {search_query}\")\n",
    "#print(f\"Generated orTerms: {orterms_for_search}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to translate query \n",
    "## payload will be adjusted to the language but we also need to aligh language of request \n",
    "\n",
    "def translate_query_for_search(query: str, target_lang: str, llm_instance) -> str:\n",
    "    \"\"\"\n",
    "    Translate search query to target language\n",
    "    \n",
    "    Args:\n",
    "        query: Original search query\n",
    "        target_lang: Target language code (en, ru, fr, ro, de)\n",
    "        llm_instance: LLM instance for translation\n",
    "        \n",
    "    Returns:\n",
    "        Translated query string\n",
    "    \"\"\"\n",
    "    language_names = {\n",
    "        \"en\": \"English\",\"ru\": \"Russian\", \"fr\": \"French\",\"ro\": \"Romanian\",\"de\":\"German\"\n",
    "    }\n",
    "\n",
    "    target_language = language_names[target_lang]\n",
    "\n",
    "    translation_prompt = f\"\"\"Translate this search query to {target_language}. \n",
    "                            Keep it concise and search-engine friendly.\n",
    "                            Keep company names in their original form (do not translate company names)\n",
    "                            Use semantically correct terms for fraud, corruption, and reputation-related concepts\n",
    "                            Only return the translated query, nothing else:\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Translation:\"\"\"\n",
    "        \n",
    "    try:\n",
    "        response = llm_instance.invoke(translation_prompt)\n",
    "        translated = response.content.strip() if hasattr(response, 'content') else str(response).strip()\n",
    "        print(f\"Translated '{query}' -> '{translated}' ({target_language})\")\n",
    "        return translated\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}. Using original query.\")\n",
    "        return query\n",
    "    \n",
    "\n",
    "result = translate_query_for_search(\" Information on Money laundering by Lukoil company \" , \"de\" , llm_translation_or_terms)\n",
    "result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79775f73",
   "metadata": {},
   "source": [
    "####  Google tool definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc777e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test google api to simulate bad request error \n",
    "\n",
    "api_key = api_google\n",
    "cse_id = SEARCH_ENGINE_ID\n",
    "\n",
    "service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "\n",
    "result = service.cse().list(\n",
    "            q=\"Lukoil Money Laundering\",\n",
    "            cx=cse_id, # search engine name defined in the console \n",
    "            dateRestrict='y2',  # Agent customizable\n",
    "            start=1,\n",
    "            filter='1', # turn on duplicated filter content \n",
    "            hl='en', # Agent customizable - user interface language\n",
    "            lr='en',  # Agent customizable - language of content             \n",
    "            num=10, # 10 i smax value\n",
    "            safe='off',\n",
    "            orTerms='Lukoil', # if incorrectly specified returned dsearch object may not contain classes\n",
    "            \n",
    "            siteSearch='en.wikipedia.org',  # Agent customizable - Wikipedia language version\n",
    "            siteSearchFilter=\"e\"  # Exclude Wikipedia\n",
    "        ).execute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b64330",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'items' in result:\n",
    "    for elem in result[\"items\"]:\n",
    "        print(elem.get('displayLink') )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common error BadRequestError: Max 20 URLs are allowed.\n",
    "# https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list#try-it \n",
    "\n",
    "## define function for tool schema\n",
    "def google_search(query,  num_results = 5, hl=\"en\", lr=\"lang_en\", dateRestrict=\"y2\", wikipedia_lang=\"en\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Search Google using Custom Search API with configurable parameters.\n",
    "    \n",
    "    This function performs Google searches and returns structured results including titles, URLs, and snippets.\n",
    "    The agent can customize language settings, time restrictions, and Wikipedia language exclusions to optimize search results for specific needs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        The search query string to execute\n",
    "    api_key : str  \n",
    "        Google Custom Search API key\n",
    "    cse_id : str\n",
    "        Google Custom Search Engine ID\n",
    "        \n",
    "    Agent Customizable Parameters:\n",
    "    -----------------------------\n",
    "\n",
    "    num_results : int, always included and defined by user\n",
    "    Number of results to return (default: 5, max: 10 per API call), Value must not exceed 10, NOT customisable\n",
    "\n",
    "\n",
    "    hl : str, always included (default: \"en\")\n",
    "        Interface language - controls UI language and affects search quality\n",
    "        Examples: \"en\", \"ru\", \"de\", \"fr\",\"ro\"\n",
    "        \n",
    "    lr : str, always included (default: \"lang_en\") \n",
    "        Content language restriction - filters results by document language\n",
    "        Examples: \"lang_en\", \"lang_ru\", \"lang_de\", \"lang_fr\"\n",
    "        \n",
    "    dateRestrict : str, always included (default: \"d365\")\n",
    "        Time-based filtering for results freshness\n",
    "        Examples: \"d1\" (past day), \"w1\" (past week), \"m1\" (past month), \n",
    "                 \"m3\" (past 3 months), \"m6\" (past 6 months), \"y1\" (past year)\n",
    "    \n",
    "    wikipedia_lang : str, always included (default: \"en\")\n",
    "        Wikipedia language version to exclude from results\n",
    "        Possible values: \"en\" (English), \"ru\" (Russian), \"fr\" (French), \"ro\" (Romanian), \"de\" (German)\n",
    "        Maps to domains: en.wikipedia.org, ru.wikipedia.org, fr.wikipedia.org, ro.wikipedia.org, de.wikipedia.org\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of dictionaries containing search results with keys:\n",
    "        - 'title': Result title\n",
    "        - 'link': Result URL  \n",
    "        - 'snippet': Result description/excerpt\n",
    "        - Additional metadata from Google API\n",
    "        \n",
    "    Example Agent Usage:\n",
    "    -------------------\n",
    "    # For Russian content excluding Russian Wikipedia\n",
    "    results = google_search(query, api_key, cse_id, hl=\"ru\", lr=\"lang_ru\", dateRestrict=\"y3\", wikipedia_lang=\"ru\")\n",
    "    \n",
    "    # For French content excluding French Wikipedia\n",
    "    results = google_search(query, api_key, cse_id, hl=\"fr\", lr=\"lang_fr\", dateRestrict=\"y3\", wikipedia_lang=\"fr\")\n",
    "    \n",
    "    # For Romanian content excluding Romanian Wikipedia\n",
    "    results = google_search(query, api_key, cse_id, hl=\"ro\", lr=\"lang_ro\", dateRestrict=\"y3\", wikipedia_lang=\"ro\")\n",
    "\n",
    "    # For German content excluding German Wikipedia\n",
    "    results = google_search(query, api_key, cse_id, hl=\"de\", lr=\"lang_de\", dateRestrict=\"y3\", wikipedia_lang=\"de\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Executing google_search_payload ...\")\n",
    "\n",
    "    api_key = api_google\n",
    "    cse_id = SEARCH_ENGINE_ID\n",
    "\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    \n",
    "    # Map language codes to Wikipedia domains\n",
    "    wikipedia_domains = {\n",
    "        \"en\": \"en.wikipedia.org\",\n",
    "        \"ru\": \"ru.wikipedia.org\", \n",
    "        \"fr\": \"fr.wikipedia.org\",\n",
    "        \"ro\": \"ro.wikipedia.org\",\n",
    "        \"de\": \"de.wikipedia.org\"\n",
    "    }\n",
    "    \n",
    "    wikipedia_site = wikipedia_domains.get(wikipedia_lang, \"en.wikipedia.org\")\n",
    "    \n",
    "    ## store search results \n",
    "    all_results = []\n",
    "   \n",
    "    ## query language must be aligned with search parameters\n",
    "    translated_query = translate_query_for_search(query , hl , llm_translation_or_terms)\n",
    "\n",
    "    ## orterms\n",
    "    orterms_for_search = extract_orterms_from_query(translated_query, llm_translation_or_terms, hl )\n",
    "    print(\"Additional search terms: \" , orterms_for_search)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        result1 = service.cse().list(\n",
    "            q=translated_query,\n",
    "            cx=cse_id, # search engine name defined in the console \n",
    "            dateRestrict=dateRestrict,  # Agent customizable\n",
    "            start=1,\n",
    "            filter='1', # turn on duplicated filter content \n",
    "            hl=hl, # Agent customizable - user interface language\n",
    "            lr=lr,  # Agent customizable - language of content             \n",
    "            num=num_results, \n",
    "            orTerms=orterms_for_search, # each document must contain at least one of the additional terms \n",
    "            safe='off',\n",
    "            siteSearch=wikipedia_site,  # Agent customizable - Wikipedia language version\n",
    "            siteSearchFilter=\"e\"  # Exclude Wikipedia\n",
    "        ).execute()\n",
    "        \n",
    "        if 'items' in result1:\n",
    "                # Extract only relevant information from each result\n",
    "                filtered_items = []\n",
    "                for item in result1['items']:\n",
    "                    essential_data = {\n",
    "                        'query':translated_query,\n",
    "                        'title': item.get('title', ''),\n",
    "                        'link': item.get('link', ''),\n",
    "                        'snippet': item.get('snippet', ''),\n",
    "                        'displayLink': item.get('displayLink', '')\n",
    "                    }\n",
    "                    filtered_items.append(essential_data)\n",
    "\n",
    "                all_results.extend(filtered_items)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting results: {e}\")\n",
    "\n",
    "    print(\"Executing google_search_payload DONE \")    \n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "\n",
    "# Message for llm to create payload \n",
    "system_message_google_search = SystemMessage(content=\"\"\"\n",
    "You are a multi-language search assistant. When asked to search for information, \n",
    "call the google_search tool ONCE for EACH of these 5 languages:\n",
    "\n",
    "1. **Romanian** (ro):\n",
    "   - hl=\"ro\" (Romanian interface)\n",
    "   - lr=\"lang_ro\" (Romanian content)\n",
    "   - wikipedia_lang=\"ro\" (exclude Romanian Wikipedia)\n",
    "\n",
    "2. **English** (en):\n",
    "   - hl=\"en\" (English interface)  \n",
    "   - lr=\"lang_en\" (English content)\n",
    "   - wikipedia_lang=\"en\" (exclude English Wikipedia)\n",
    "\n",
    "3. **Russian** (ru):\n",
    "   - hl=\"ru\" (Russian interface)\n",
    "   - lr=\"lang_ru\" (Russian content) \n",
    "   - wikipedia_lang=\"ru\" (exclude Russian Wikipedia)\n",
    "\n",
    "4. **French** (fr):\n",
    "   - hl=\"fr\" (French interface)\n",
    "   - lr=\"lang_fr\" (French content)\n",
    "   - wikipedia_lang=\"fr\" (exclude French Wikipedia)\n",
    "                            \n",
    "5. **German** (de):\n",
    "   - hl=\"de\" (German interface)\n",
    "   - lr=\"lang_de\" (German content)\n",
    "   - wikipedia_lang=\"de\" (exclude German Wikipedia)\n",
    "\n",
    "IMPORTANT: Make exactly 10 tool calls (one per language). \n",
    "For each call, use num_results = 10 to retrieve 10 search results per language.\n",
    "Use dateRestrict=\"y2\" (last year) for recent information.\n",
    "\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test function \n",
    "tool_google_search = StructuredTool.from_function( google_search, name = \"google_search\" )\n",
    "\n",
    "entity_name = \"Lukoil\"\n",
    "query = f\"{entity_name} money laundering compliance violations investigations\"\n",
    "\n",
    "# join instructions and entity name\n",
    "messages = [ system_message_google_search ,  HumanMessage(content=query) ]\n",
    "\n",
    "## create list of tools, which be called externally with  tool_call['args']\n",
    "tools = [ tool_google_search  ]\n",
    "tools_by_name = {tool.name:tool for tool in tools}\n",
    "\n",
    "# bind first tool to 4o mnodel\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# test tool call without payload \n",
    "payload = llm_with_tools.invoke( messages )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check payload values\n",
    "for tool_call in payload.tool_calls:\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"ID: {tool_call['id']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")\n",
    "    print(\"---\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function  using single payload\n",
    "#tool_call = payload.tool_calls[0]\n",
    "#tool_google_search.invoke( tool_call['args'] )\n",
    "#print(\"##-----------------##\")\n",
    "\n",
    "## Now run through app payloads  \n",
    "#for tool_call in payload.tool_calls: # AI message with payload , last ai message with too l calls\n",
    "#        tool = tools_by_name[tool_call[\"name\"]]\n",
    "#        observations = tool.invoke(tool_call[\"args\"])\n",
    "#        print(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## google search tool execution and allocation of seatch results\n",
    "def tool_node_chunk_selection_exec(state: UnifiedResearchState):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    print(\"Executing tool_node_chunk_selection_exec...\")\n",
    "    \n",
    "    result = []\n",
    "    link_collections = []\n",
    "\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls: # AI message with payload , last ai message with too l calls\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "           \n",
    "        result.append(ToolMessage( \n",
    "                       content = observation, \n",
    "                       tool_call_id=tool_call[\"id\"] , \n",
    "                       name = tool_call['name']))    \n",
    "        \n",
    "        # Extract LinkCollection data from observation\n",
    "        for item in observation:\n",
    "            if isinstance(item, dict):\n",
    "                # Check if required keys exist\n",
    "                if 'displayLink' in item and 'link' in item:\n",
    "                    link_collection = LinkCollection(\n",
    "                        displayLink=item['displayLink'],\n",
    "                        link=item['link']\n",
    "                    )\n",
    "                    link_collections.append(link_collection)\n",
    "                else:\n",
    "                    # This will only print for items that are missing keys\n",
    "                    available_keys = list(item.keys())\n",
    "                    print(f\"Item missing required keys. Available keys: {available_keys}\")\n",
    "                    print(f\"Item content: {item}\")\n",
    "            else:\n",
    "                print(f\"Item is not a dict, it's: {type(item)}\")\n",
    "        \n",
    "    # we need to populate LinkCollection class and store in  search_results \n",
    "\n",
    "    print(\"Executing tool_node_chunk_selection_exec END\")\n",
    "    return {\"messages\": result , \"search_results\":link_collections }   \n",
    "\n",
    "\n",
    "## Node to create payload \n",
    "def llm_node(state: UnifiedResearchState):\n",
    "    \"\"\"LLM node that generates tool calls\"\"\"\n",
    "    print(\"Executing llm_node...\")\n",
    "    \n",
    "    # Get the last message from state\n",
    "    entity_name  = state[\"messages\"][-1] # human message\n",
    "    query = f\"{entity_name.content} money laundering criminal activity\"\n",
    "    \n",
    "    ## modified content \n",
    "    human_message_updated = HumanMessage(content=query, id=entity_name.id)\n",
    "    print(\"Updated human message: \" , human_message_updated.content)\n",
    "\n",
    "    ## Replace content of the original human imput \n",
    "    system_message = system_message_google_search\n",
    "\n",
    "    messages = [ system_message ,  human_message_updated ]\n",
    "    \n",
    "    # Call LLM with tools\n",
    "    response = llm_with_tools.invoke( messages )\n",
    "    print(f\"LLM response: {response}\")\n",
    "    print(\"State fields \" , state)\n",
    "    \n",
    "    # return all up to last message, replace last human input , company name(for prompts), expanded query for debugging in English\n",
    "    return {\"messages\": state[\"messages\"][:-1] + [human_message_updated, response], \"entity_name\": entity_name.content , \"expanded_query\":query }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to display flow using cli\n",
    "\n",
    "def render_mermaid_graph(mermaid_code, output_filename='graph.png', width=700, height=700, cleanup=True):\n",
    "    \"\"\"\n",
    "    Render Mermaid diagram using CLI and display in Jupyter\n",
    "    \"\"\"\n",
    "    # Create temporary mermaid file\n",
    "    temp_mmd = output_filename.replace('.png', '.mmd').replace('.svg', '.mmd').replace('.pdf', '.mmd')\n",
    "    \n",
    "    try:\n",
    "        # Write mermaid code to file\n",
    "        with open(temp_mmd, 'w', encoding='utf-8') as f:\n",
    "            f.write(mermaid_code)\n",
    "        \n",
    "        # Build command based on output format\n",
    "        # Use the Windows .cmd version\n",
    "        cmd = [\n",
    "            'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\npm\\\\mmdc.cmd',  # Full path to Windows version\n",
    "            '-i', temp_mmd,\n",
    "            '-o', output_filename,\n",
    "            '-w', str(width),\n",
    "            '-H', str(height)\n",
    "        ]\n",
    "        \n",
    "        # Add size parameters only for PNG/PDF\n",
    "        if output_filename.endswith(('.png', '.pdf')):\n",
    "            cmd.extend(['-w', str(width), '-H', str(height), '--scale', '2'])\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True) # we simulate running command like\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"Graph rendered successfully: {output_filename}\")\n",
    "            try:\n",
    "                display(Image(output_filename))\n",
    "            except:\n",
    "                print(f\"Image saved to {output_filename} but could not display inline\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Mermaid CLI error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"Mermaid CLI not found. Install with: npm install -g @mermaid-js/mermaid-cli\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering graph: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if cleanup and os.path.exists(temp_mmd):\n",
    "            os.remove(temp_mmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b6ba5",
   "metadata": {},
   "source": [
    "#### Prepare Tavily for content extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate summary of extracted content\n",
    "\n",
    "def generate_url_summary(raw_content: str, llm, entity_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a focused summary of raw content with emphasis on company reputation \n",
    "    and criminal activity using dedicated LLM.\n",
    "    \n",
    "    Args:\n",
    "        raw_content: Text content to analyze\n",
    "        llm: Language model instance for processing\n",
    "        entity_name: Company name for context\n",
    "        \n",
    "    Returns:\n",
    "        str: Focused summary or error message\n",
    "        \n",
    "    Raises:\n",
    "        None: All exceptions are caught and handled gracefully\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not raw_content or not raw_content.strip():\n",
    "        return \"Error: No content provided for analysis\"\n",
    "    \n",
    "    if not entity_name or not entity_name.strip():\n",
    "        return \"Error: Entity name is required\"\n",
    "    \n",
    "    # Truncate content if too long (prevent token limit issues)\n",
    "    max_content_length = 5000  # Adjust based on your model's limits\n",
    "    if len(raw_content) > max_content_length:\n",
    "        raw_content = raw_content[:max_content_length] + \"...[truncated]\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            You are analyzing an article about {entity_name} company for anti-money laundering due diligence.\n",
    "\n",
    "            Extract and summarize ANY information related to:\n",
    "\n",
    "            FINANCIAL CRIMES (primary focus):\n",
    "            - Money laundering allegations, investigations, charges, or convictions\n",
    "            - AML program deficiencies or regulatory violations\n",
    "            - Suspicious transaction monitoring failures\n",
    "            - Know Your Customer (KYC) violations\n",
    "\n",
    "            RELATED VIOLATIONS (also capture):\n",
    "            - Sanctions violations (OFAC, EU, UN sanctions lists)\n",
    "            - Tax evasion or aggressive tax avoidance schemes\n",
    "            - Fraud, embezzlement, or misappropriation of funds\n",
    "            - Corruption, bribery, or kickbacks\n",
    "            - Securities violations involving misrepresentation\n",
    "\n",
    "            COMPLIANCE ACTIONS (positive or negative):\n",
    "            - Regulatory fines, penalties, or enforcement actions\n",
    "            - Legal proceedings, settlements, or court rulings\n",
    "            - Compliance program implementations or improvements\n",
    "            - Internal investigations or audits\n",
    "            - Management changes related to compliance issues\n",
    "\n",
    "            CRITICAL INSTRUCTIONS:\n",
    "            1. **Distinguish allegation vs. fact**: \n",
    "            - \"Company accused of...\" (allegation)\n",
    "            - \"Company convicted of...\" (proven fact)\n",
    "            - \"Investigation closed without charges\" (exoneration)\n",
    "\n",
    "            2. **Include legal status**:\n",
    "            - Ongoing investigation\n",
    "            - Settled with fine\n",
    "            - Dismissed by court\n",
    "            - Under appeal\n",
    "\n",
    "            3. **Capture both negative AND positive**:\n",
    "            - Violations AND settlement efforts\n",
    "            - Fines paid AND compliance improvements implemented\n",
    "\n",
    "            4. **For irrelevant articles**: \n",
    "            \"Article discusses {entity_name} [topic] but contains no financial crime or compliance information.\"\n",
    "\n",
    "            5. **Always include**:\n",
    "            - Date of article or events (if available)\n",
    "            - Source publication\n",
    "            - Specific amounts (fines, transactions, etc.)\n",
    "            - Regulatory bodies involved (FinCEN, OFAC, DOJ, etc.)\n",
    "\n",
    "            FORMAT:\n",
    "            Date: [publication date or event date] | Source: [publication] | Legal Status: [ongoing/settled/dismissed/alleged] | [factual summary]\n",
    "\n",
    "            EXAMPLES:\n",
    "\n",
    "            Good - captures nuance:\n",
    "            \"Date: 2023-05-15 | Source: Reuters | Legal Status: Settled | FinCEN fined company €50M for AML violations during 2018-2020 related to Estonian branch operations. Settlement included admission of inadequate transaction monitoring. Company implemented new AML program in 2022 with independent monitor.\"\n",
    "\n",
    "            Good - shows exoneration:\n",
    "            \"Date: 2024-01-10 | Source: Financial Times | Legal Status: Dismissed | DOJ investigation into alleged sanctions violations closed after 18-month probe. No charges filed. Company cleared of all allegations.\"\n",
    "\n",
    "            Good - shows ongoing risk:\n",
    "            \"Date: 2024-03-20 | Source: WSJ | Legal Status: Ongoing | Company executives under investigation by EU authorities for potential money laundering related to Russian transactions 2021-2022. Company denies wrongdoing. Investigation ongoing, no charges filed yet.\"\n",
    "\n",
    "            Good - irrelevant article:\n",
    "            \"Date: 2024-02-15 | Source: Bloomberg | Article discusses {entity_name}'s Q4 earnings and CEO succession but contains no financial crime or compliance information.\"\n",
    "\n",
    "            Bad - too vague:\n",
    "            \"Company has compliance issues.\" \n",
    "            (Missing: dates, specifics, amounts, legal status, remediation)\n",
    "\n",
    "            Content to analyze:\n",
    "            {raw_content}\n",
    "\n",
    "            Summary (follow format above):\n",
    "            \"\"\"\n",
    "           \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Validate response\n",
    "        if not response or not hasattr(response, 'content'):\n",
    "            return \"Error: Invalid response from language model\"\n",
    "        \n",
    "        summary = response.content.strip()\n",
    "        \n",
    "        # Ensure we got actual content\n",
    "        if not summary:\n",
    "            return \"Error: Empty response from language model\"\n",
    "            \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        # More specific error handling\n",
    "        error_msg = f\"Error generating reputation summary: {str(e)}\"\n",
    "        print(error_msg)  # For debugging\n",
    "        return \"Error: Could not generate reputation summary due to processing issues\"\n",
    "\n",
    "\n",
    "# Fixed test call\n",
    "result = generate_url_summary(\n",
    "    \"Lukoil was accused of money laundering in 2023. The company faced several lawsuits and fines related to financial misconduct. However, it has not been involved in any major scandals or controversies recently.\",\n",
    "    llm_url_content,\n",
    "    \"Lukoil\"  # Added missing entity_name parameter\n",
    ")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client once at module level\n",
    "tavily_client = TavilyClient(api_key=tavily_api)\n",
    "#  avily API has a limit of 20 URLs per extraction request\n",
    "# we can face error bad requiest max 20 is allowed\n",
    "\n",
    "# function will rin inside the node, tool is not generated \n",
    "# as application is strainghforward \n",
    "\n",
    "def tavily_content_extractor(\n",
    "    dummy_state: List[LinkCollection],    \n",
    "    extract_depth: str = \"basic\",\n",
    "    include_raw_content: bool = True,\n",
    "    entity_name:str = \"\"\n",
    "\n",
    ") -> List[LinkCollection]:\n",
    "    \n",
    "    print(\"Executing content extraction...\")\n",
    "\n",
    "    # store urls values\n",
    "    urls = [link.link for link in dummy_state]\n",
    "\n",
    "    ## extract all urls\n",
    "    BATCH_SIZE = 20 \n",
    "    url_to_content = {} # need to store \"url\" and \"raw_content\"\n",
    "\n",
    "    for i in range(0, len(urls), BATCH_SIZE ):\n",
    "        batch_urls = urls[i: ( i + BATCH_SIZE ) ]\n",
    "        print(f\"Processing batch {i//BATCH_SIZE + 1}: {len(batch_urls)} URLs\")\n",
    "\n",
    "        try: \n",
    "            response = tavily_client.extract(urls=batch_urls, extract_depth=extract_depth)     \n",
    "\n",
    "            # Map URLs to content from this batch\n",
    "            for item in response.get('results', []):\n",
    "                url = item.get(\"url\", \"\")\n",
    "                content = item.get(\"raw_content\", \"\")\n",
    "                url_to_content[url] = content  \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting batch: {e}\")\n",
    "            # Continue with empty content for failed URLs\n",
    "            for url in batch_urls:\n",
    "                if url not in url_to_content:\n",
    "                    url_to_content[url] = \"\"        \n",
    "\n",
    "    \n",
    "    # Create new LinkCollection objects with matched raw_content\n",
    "    updated_collections = []\n",
    "    for link_collection in dummy_state:\n",
    "        raw_content = url_to_content.get(link_collection.link, \"\")\n",
    "        \n",
    "        if raw_content:  # may not be present for pdf \n",
    "\n",
    "            # generate summary\n",
    "            print(f\"Generate summary for URL: {link_collection.displayLink}\")\n",
    "            raw_content_summary = generate_url_summary(raw_content, llm_url_content , entity_name) \n",
    "        else:\n",
    "            print(f\"No content found for URL: {link_collection.displayLink}\")\n",
    "            raw_content_summary = \"\"\n",
    "\n",
    "        updated_collection = LinkCollection(\n",
    "            displayLink=link_collection.displayLink,\n",
    "            link=link_collection.link,\n",
    "            raw_content=raw_content,\n",
    "            summary=raw_content_summary\n",
    "        )\n",
    "        \n",
    "        updated_collections.append(updated_collection)\n",
    "\n",
    "       \n",
    "    print(\"Executing content extraction Done\")\n",
    "\n",
    "    return updated_collections\n",
    "\n",
    "# Test \n",
    "#dummy = tavily_content_extractor(dummy_state=results[\"search_results\"],  entity_name= results[\"entity_name\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function above will not be implemented as tool \n",
    "# we will update search_results component for each LinkCollection\n",
    "\n",
    "def extract_content(state: UnifiedResearchState) -> UnifiedResearchState :\n",
    "    \"\"\"Node function to extract content from URLs in search results\"\"\"\n",
    "    \n",
    "    print(\"Executing extract_content...\")\n",
    "    \n",
    "    dummy_state = state[\"search_results\"].copy()\n",
    "    updated_search_result = tavily_content_extractor(dummy_state=dummy_state , entity_name = state[\"entity_name\"])\n",
    "    \n",
    "    print(\"Extract_content completed Done\")\n",
    "\n",
    "    return {\"search_results\": updated_search_result} # replace the searchresult field\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a76743",
   "metadata": {},
   "source": [
    "#### Unite Research and HyDe agent into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## input comes from expanded query formed after llm node\n",
    "\n",
    "## now define nodes \n",
    "\n",
    "journalist_instructions = \"\"\"You are tasked with creating exactly {max_journalists} AI \n",
    "journalist personas who will analyze the same financial crime topic from different critical perspectives for banking risk assessment. \n",
    "\n",
    "Topic under investigation: {crime_topic}\n",
    "\n",
    "Create {max_journalists} journalists who each bring a unique lens to evaluate whether a bank should collaborate with a B2B company:\n",
    "\n",
    "1. **Regulatory Compliance Analyst**: Focuses on regulatory violations, sanctions, and legal compliance issues that would trigger regulatory action against the bank.\n",
    "\n",
    "2. **Financial Due Diligence Investigator**: Examines the company's financial integrity, transaction patterns, and money flow irregularities that pose reputational risk.\n",
    "\n",
    "3. **Operational Risk Assessor**: Analyzes operational red flags, corporate governance failures, and business practice irregularities that indicate systemic risk.\n",
    "\n",
    "4. **Reputational Risk Evaluator**: Investigates media coverage, public scandals, and association risks that could damage the bank's reputation through partnership.\n",
    "\n",
    "5. **KYC/AML Specialist**: Focuses on Know Your Customer failures, suspicious beneficial ownership, and anti-money laundering concerns that create compliance liability.\n",
    "\n",
    "For each journalist:\n",
    "- expertise: Their specific risk assessment specialization\n",
    "- perspective: How they evaluate partnership risks from their expert viewpoint  \n",
    "- style: Their analytical approach to uncovering risks relevant to banking partnerships\n",
    "\n",
    "Return all journalists in the HydePerspectives format.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def create_journalists( state:UnifiedResearchState  ) :\n",
    "     \n",
    "     \"\"\" create journalist \"\"\" \n",
    "\n",
    "     print(\"Executing create_journalists ... \")\n",
    "\n",
    "      # prompts arguments\n",
    "     crime_topic=state['expanded_query']\n",
    "     max_journalists=state['max_journalists']\n",
    "     \n",
    "     # llm to generate persona \n",
    "     structured_llm = llm_class_generation.with_structured_output( HydePerspectives )\n",
    "\n",
    "     system_message = journalist_instructions.format( crime_topic = crime_topic , \n",
    "                                                      max_journalists= max_journalists\n",
    "                                                     )\n",
    "     print( \"System Instructions: \" , system_message[:100])\n",
    "     \n",
    "     # populate values  \n",
    "     journalists = structured_llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=\"Generate the set of journalists.\")])\n",
    "\n",
    "     print(\"Executing create_journalists Done \")\n",
    "\n",
    "     return { \"journalists\": journalists.journalists }\n",
    "\n",
    "\n",
    "\n",
    "## function to write article \n",
    "def generate_hyde_document(state:UnifiedResearchState): \n",
    "\n",
    "    \"\"\"Generate Hyde articles using journalist personas\"\"\"\n",
    "\n",
    "    print(\"Executing generate_hyde_document ... \")\n",
    "\n",
    "    article_template = \"\"\"You are a journalist with the following profile:\n",
    "\n",
    "            Expertise: {expertise}\n",
    "            Perspective: {perspective}\n",
    "            Style: {style}\n",
    "\n",
    "            Topic: {crime_topic}\n",
    "\n",
    "            Write a hypothetical article about this crime_topic that authentically reflects your expertise, \n",
    "            perspective, and writing style. The article should be 200-300 words and demonstrate your unique approach to covering this story.\"\"\"\n",
    "            \n",
    "    hyde_list = []\n",
    "\n",
    "    print(\"Unpacking journalists classes\")\n",
    "\n",
    "    ## unpack values in journalists field\n",
    "    journalists =  [ state[\"journalists\"] ] \n",
    "\n",
    "    for elem in state['journalists']:\n",
    "        prompt = article_template.format(\n",
    "            expertise=elem.expertise,\n",
    "            perspective=elem.perspective, \n",
    "            style=elem.style,\n",
    "            crime_topic=state[\"expanded_query\"]\n",
    "        )\n",
    "\n",
    "        article = llm_hyde_generation.invoke([HumanMessage(content=prompt)])\n",
    "        hyde_list.append(article.content)\n",
    "\n",
    "    print(\"Executing generate_hyde_document Done \")    \n",
    "\n",
    "    return {\"hyde_list\": hyde_list}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e23573",
   "metadata": {},
   "source": [
    "#### Prepare module for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok test comparison algo , each article is compared against\n",
    "## HeDe docs\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# we compare 1 article agains max_journalist , the format is ( n_row , n_journalists ) \n",
    "\n",
    "text_samples = [\n",
    "    \"The financial institution faces significant regulatory compliance challenges and potential money laundering violations that could impact its banking partnerships.\",\n",
    "    \n",
    "    \"Recent investigations reveal operational risk factors including inadequate anti-money laundering controls and suspicious transaction monitoring failures.\",\n",
    "    \n",
    "    \"Market analysts express concerns about reputational damage from ongoing legal proceedings and regulatory scrutiny affecting stakeholder confidence.\"\n",
    "]\n",
    "\n",
    "url_content = text_samples[1]\n",
    "\n",
    "## form embedding of original \n",
    "\n",
    "vector_url_content = np.array( embedding_cross_lang.embed_query(url_content) )\n",
    "vector_url_content = vector_url_content.reshape(1,-1)\n",
    "print(vector_url_content.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function will be iteravely applied over each content \n",
    "\n",
    "def check_content_similarity(\n",
    "    search_results: List[LinkCollection], \n",
    "    hyde_list: npt.NDArray[np.float64]\n",
    ") -> List[LinkCollection]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if URL content has high similarity to any Hyde document\n",
    "    \n",
    "    Args:\n",
    "        search_results: List of LinkCollection objects with raw_content\n",
    "        hyde_list: numpy array of shape (n, 1536) containing Hyde embeddings\n",
    "    \n",
    "    Returns:\n",
    "        float: Maximum cosine similarity score rounded to hundredth (2 decimal places)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store max similarity score for each URL\n",
    "    updated_collections = []  # stroe new values for the collection, later replace, solution is redundant\n",
    "    for link in search_results:\n",
    "\n",
    "        url_content = link.raw_content\n",
    "        \n",
    "        # Embed URL content, cap is 300 000 tokens, 1 token is aproximately 4 characters, Embedding error: Error code: 400 - {'error': {'message': 'Requested 307281 tokens, max 300000 tokens per request', 't\n",
    "        try:\n",
    "           url_content_embed = np.array(\n",
    "               embedding_cross_lang.embed_query(url_content[:800000])\n",
    "           ).reshape(1, -1)\n",
    "        except Exception as e:\n",
    "           print(f\"Embedding error: {str(e)[:100]}\")\n",
    "           url_content_embed = np.zeros((1, 3072))  # Default embedding for text-embedding-3-large\n",
    "        \n",
    "        # calculate similarity scores against all Hyde embeddings\n",
    "        similarity_scores = cosine_similarity(url_content_embed, hyde_list)\n",
    "        \n",
    "        # Find and store max similarity for this URL\n",
    "        max_score = round(float(np.max(similarity_scores)), 2)\n",
    "        \n",
    "       \n",
    "        # update the state values , fix later \n",
    "        updated_collection = LinkCollection(\n",
    "            displayLink=link.displayLink,\n",
    "            link=link.link,\n",
    "            raw_content=link.raw_content,\n",
    "            summary=link.summary,\n",
    "            hyde_score=max_score\n",
    "        )\n",
    "        \n",
    "        updated_collections.append(updated_collection)\n",
    "    \n",
    "    return updated_collections\n",
    "\n",
    "# Usage:\n",
    "#results[\"search_results\"] = check_content_similarity(\n",
    "#    results[\"search_results\"], \n",
    "#    hyde_content_list_embed\n",
    "#)\n",
    "\n",
    "# Tools are for external API, actions which requires LLM to decide parameters , sityation whcih required dynamic parameters\n",
    "# Here we process data which already in a state, no external api, state modification, sequential processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7587e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Node to update  state[\"search_results\"] and keep only relevant material for summary \n",
    "\n",
    "def filter_relevant_content(state: UnifiedResearchState) -> UnifiedResearchState:\n",
    "    \"\"\"Filter and sort search results by HyDE relevance score\"\"\"\n",
    "    \n",
    "    print(\"Executing filter_relevant_content...\")\n",
    "    \n",
    "    RELEVANCE_THRESHOLD = 0.3  # Configurable\n",
    "    TOP_K = 15  # Maximum articles to keep\n",
    "    \n",
    "    # Filter items with scores\n",
    "    scored_items = [\n",
    "        link for link in state[\"search_results\"]\n",
    "        if link.hyde_score is not None\n",
    "    ]\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    sorted_items = sorted(\n",
    "        scored_items, \n",
    "        key=lambda x: x.hyde_score, \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Apply threshold and limit\n",
    "    filtered = [\n",
    "        item for item in sorted_items\n",
    "        if item.hyde_score >= RELEVANCE_THRESHOLD\n",
    "    ][:TOP_K]\n",
    "    \n",
    "    # Diagnostic output\n",
    "    print(f\"Total articles: {len(state['search_results'])}\")\n",
    "    print(f\"Articles with scores: {len(scored_items)}\")\n",
    "    print(f\"Above threshold ({RELEVANCE_THRESHOLD}): {len(filtered)}\")\n",
    "    \n",
    "    if filtered:\n",
    "        scores = [item.hyde_score for item in filtered]\n",
    "        print(f\"Score range: {min(scores):.2f} - {max(scores):.2f}\")\n",
    "        print(f\"Top 5 sources: {[item.displayLink[:30] for item in filtered[:5]]}\")\n",
    "    else:\n",
    "        print(\"WARNING: No articles passed relevance threshold!\")\n",
    "    \n",
    "    print(\"Executing filter_relevant_content Done\")\n",
    "    \n",
    "    return {\"filtered_results\": filtered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c058482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now prepare function to integrate into main flow \n",
    "\n",
    "# with cosine we should distingusin \n",
    "#company accused in money laundering, mentioned in article about laundering\n",
    "# denying accusations, complying with anti money policy\n",
    "\n",
    "def assign_score_vs_hyde(state: UnifiedResearchState) -> UnifiedResearchState :\n",
    "    \"\"\"Node function to estimate similarity score agains HyDe documents\"\"\"\n",
    "    \n",
    "    print(\"Executing assign_score_vs_hyde...\")\n",
    "\n",
    "    ## prepare HydeEmbeddings \n",
    "    hyde_content_list = state.get(\"hyde_list\", None)\n",
    "    # hyde_content_list_embed = np.vstack([  np.array( embedding_cross_lang.embed_query(article) ).reshape(1,-1) for article in hyde_content_list ])\n",
    "\n",
    "    hyde_embeddings = embedding_cross_lang.embed_documents(hyde_content_list)\n",
    "    hyde_content_list_embed = np.array(hyde_embeddings)\n",
    "    \n",
    "    ## prepare input for score estimation\n",
    "    dummy_state = state[\"search_results\"].copy()\n",
    "    updated_search_result = check_content_similarity(search_results=dummy_state , hyde_list = hyde_content_list_embed )\n",
    "    \n",
    "    print(\" Executing assign_score_vs_hyde  Done\")\n",
    "\n",
    "    return {\"search_results\": updated_search_result} # replace the searchresult field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0d49c",
   "metadata": {},
   "source": [
    "#### Evidence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa14340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need verifiation that final assessment clains are based on the evidence \n",
    "# collected as summaries \n",
    "# We need to emplly evidence therefore add modules to trace the process\n",
    "\n",
    "# Eventually, evidences must be collected into reusable sharable format \n",
    "# It should help to avoid hallusination and back up arguments with evidences \n",
    "\n",
    "#llm_claim_extraction = ChatOpenAI(\n",
    "#    model=\"gpt-5\",        # gpt 5 has higher latency, frequent limit hits \n",
    "#    temperature=0.1,      # Zero for deterministic extraction\n",
    "#    max_tokens=7000,      # Enough for multiple claims\n",
    "#    timeout=60,           # Complex consolidation takes time\n",
    "#    api_key=api_key_var\n",
    "#)\n",
    "# we require consistent deduplication, inproved output structure with more discinplined extraction, lower hallucination\n",
    "# LengthFinishReasonError: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=4000, prompt_tokens=3174, total_tokens=7174,\n",
    "\n",
    "\n",
    "## test the class \n",
    "\n",
    "def extract_evidence_claims(state:UnifiedResearchState) -> Dict: \n",
    "    \n",
    "    \"\"\"Extract claims from summaries and consolidate duplicate claims across sources\"\"\"\n",
    "    \n",
    "    print(\"Extracting and consolidating evidence claims...\")\n",
    "\n",
    "    # prepare summaries \n",
    "    summaries_data = [] # each element is dictionary wish summary data \n",
    "\n",
    "    for link in state[\"filtered_results\"]: \n",
    "        if link.summary.strip(): \n",
    "            summaries_data.append(\n",
    "                {\n",
    "                \"url\": link.link,\n",
    "                \"source\": link.displayLink,\n",
    "                \"summary\": link.summary, # contains date\n",
    "                \"hyde_score\": link.hyde_score\n",
    "                }\n",
    "            )\n",
    "\n",
    "    extraction_prompt = f\"\"\"\n",
    "    Extract financial crime claims about {state[\"entity_name\"]} from news summaries.\n",
    "    \n",
    "    CONTEXT: These summaries were filtered using HyDE (Hypothetical Document Embeddings) scoring.\n",
    "    The hyde_score indicates relevance (0-1 scale) by comparing each article against hypothetical \n",
    "    articles written by financial crime experts about {state[\"entity_name\"]}'s money laundering risks.\n",
    "    Higher scores (>0.5) suggest strong relevance to AML/compliance topics.\n",
    "    \n",
    "    FILTERING RULES:\n",
    "    1. ONLY extract claims about {state[\"entity_name\"]} - ignore any other companies mentioned\n",
    "    2. If a summary discusses another company's investigation/scandal, skip it entirely\n",
    "    3. Skip summaries stating \"no information about {state[\"entity_name\"]}\"\n",
    "    4. Split multi-date summaries into separate claims (some contain 5-8 different events)\n",
    "    \n",
    "    CONSOLIDATION RULES:\n",
    "    When multiple sources report the same event (even with different wording):\n",
    "    - Create ONE unified claim\n",
    "    - Combine ALL supporting URLs into one list\n",
    "    - Preserve ALL source summary texts\n",
    "    - Count independent sources correctly\n",
    "    \n",
    "    For each UNIQUE claim, provide:\n",
    "    \n",
    "    claim_text: \n",
    "    - Include specific details: amounts, dates, agencies involved\n",
    "    - Example: \"Company paid €50M fine to FinCEN in May 2023 for AML violations\"\n",
    "    \n",
    "    claim_type:   THIS IS IMPORTANT, SELECT ONLY ONE OF THIS PER CLAIM:\n",
    "    - \"fine\" → monetary penalties (paid or agreed to pay)\n",
    "    - \"investigation\" → active regulatory/criminal probes\n",
    "    - \"allegation\" → unproven accusations or claims\n",
    "    - \"clearance\" → dismissed cases, exonerations, closed without charges\n",
    "    - \"settlement\" → negotiated resolutions with authorities or plaintiffs (e.g., consent orders, DPAs/NPAs, civil settlements)\n",
    "    THIS IS IMPORTANT, SELECT ONLY ONE OF THIS\n",
    "    \n",
    "    supporting_urls: List of ALL URLs mentioning this specific claim\n",
    "    \n",
    "    date_publish: Date of claim publication\n",
    "\n",
    "    EXAMPLE CONSOLIDATION:\n",
    "    Three sources mention same fine with variations:\n",
    "    - Source A: \"agreed to pay €1.5B for AML failures\"  \n",
    "    - Source C: \"€1.5 billion fine for money laundering\"\n",
    "    \n",
    "    Result → ONE claim with:\n",
    "    - claim_text: \"{state[\"entity_name\"]} paid €1.5 billion fine to settle AML violations\"\n",
    "    - claim_type: \"fine\"\n",
    "    - supporting_urls: [urlA, urlB, urlC]\n",
    "    - date_publish: \"2023-12-23\"\n",
    "    \n",
    "    EXTRACTION PRIORITIES:\n",
    "    - Focus on the most serious violations and largest fines first\n",
    "    - Capture the full timeline of events (earliest to latest)\n",
    "    - Include both negative findings AND positive remediation efforts\n",
    "    - Distinguish between different regulatory jurisdictions (US, EU, national)\n",
    "    \n",
    "    Summaries to analyze:\n",
    "    {json.dumps(summaries_data, indent=2)}\n",
    "    \n",
    "    Return only consolidated, unique claims about {state[\"entity_name\"]}.\n",
    "    \"\"\"\n",
    "    # debug\n",
    "    #print(json.dumps(summaries_data, indent=2))\n",
    "\n",
    "\n",
    "    structured_llm = llm_summarization.with_structured_output( ClaimsFromSummaries )\n",
    "\n",
    "    response = structured_llm.invoke([\n",
    "        SystemMessage(content=\"You are a forensic analyst consolidating evidence.\"),\n",
    "        HumanMessage(content=extraction_prompt)\n",
    "    ])\n",
    "\n",
    "    print(f\"Extracted {len(response.evidence_claims)} unique claims from {len(summaries_data)} summaries\")\n",
    "    # json dums return string which is applied to promot via f syntax\n",
    "\n",
    "    print(response.evidence_claims)\n",
    "    return {\"evidence_claims\" : response.evidence_claims}\n",
    "\n",
    "#buff = extract_evidence_claims(results)  # tested \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust summarisation node \n",
    "\n",
    "def generate_risk_assessment(state: UnifiedResearchState):\n",
    "    \"\"\"Generate comprehensive AML risk assessment from evidence claims\"\"\"\n",
    "    \n",
    "    print(\"Executing generate_risk_assessment...\")\n",
    "    \n",
    "    # Group claims by type for analysis\n",
    "    claims_by_type = {\n",
    "        \"fine\": [],\n",
    "        \"investigation\": [],\n",
    "        \"allegation\": [],\n",
    "        \"clearance\": [],\n",
    "        \"settlement\": []\n",
    "    }\n",
    "\n",
    "    print(\"State keys\" , state.keys())\n",
    "    types = {c.claim_type for c in state[\"evidence_claims\"]}\n",
    "    print(\"Claim types:\", sorted(types)) # myst be fine investigation, allegation, clearance, settlement\n",
    "    \n",
    "    print(\"Iterate through claim cases: \")\n",
    "    for claim in state[\"evidence_claims\"]:\n",
    "        claims_by_type[claim.claim_type].append({\n",
    "            \"text\": claim.claim_text,\n",
    "            \"sources\": len(claim.supporting_urls),\n",
    "            \"date_publish\":claim.date_publish\n",
    "        })\n",
    "    \n",
    "    entity_name = state[\"entity_name\"]\n",
    "    \n",
    "    system_message = SystemMessage(content=f\"\"\"\n",
    "    You are a senior AML compliance officer conducting risk assessment for {entity_name}.\n",
    "    \n",
    "    Analyze the evidence to determine banking partnership viability.\n",
    "    \n",
    "    KEY ASSESSMENT CRITERIA:\n",
    "    \n",
    "    1. VIOLATION SEVERITY\n",
    "    - Calculate total fines from the evidence (don't hardcode)\n",
    "    - Identify the scale of violations (amounts involved vs fines paid)\n",
    "    - Determine if violations were systemic or isolated\n",
    "    - Check if company admitted guilt or just settled\n",
    "    \n",
    "    2. CURRENT RISK STATUS\n",
    "    - Count active investigations (ongoing = unresolved risk)\n",
    "    - Identify investigating authorities (DOJ, EU, FATF = high concern)\n",
    "    - Assess geographic spread (multiple jurisdictions = higher risk)\n",
    "    - Evaluate timeline (recent violations = weak current controls)\n",
    "    \n",
    "    3. CONTROL ENVIRONMENT\n",
    "    - Evaluate settlement efforts (genuine improvement vs PR)\n",
    "    - Check for leadership accountability (executives resigned?)\n",
    "    - Assess system improvements (new AML systems, training?)\n",
    "    - Look for independent validation (monitors, audits)\n",
    "    \n",
    "    4. PATTERN RECOGNITION\n",
    "    - Multiple fines for similar issues = poor compliance culture\n",
    "    - Repeated violations across years = systemic failure\n",
    "    - Mix of old violations + strong settlement = possible reform\n",
    "    - Only allegations with no convictions = lower concern\n",
    "    \n",
    "    DECISION FRAMEWORK:\n",
    "    \n",
    "    AVOID PARTNERSHIP if:\n",
    "    - Criminal investigations ongoing\n",
    "    - Multiple violations without meaningful settlement\n",
    "    - Pattern of violations continuing to present\n",
    "    - Total fines exceed €1 billion with no improvement\n",
    "    \n",
    "    ENHANCED DUE DILIGENCE if:\n",
    "    - Significant past violations but settled\n",
    "    - Active settlement program with evidence\n",
    "    - Civil investigations ongoing (not criminal)\n",
    "    - Mix of violations and clearances\n",
    "    \n",
    "    \n",
    "    REQUIRED OUTPUT:\n",
    "    1. EXECUTIVE SUMMARY: Core issues and total financial impact (calculate from evidence)\n",
    "    2. VIOLATION ANALYSIS: What they did wrong and when\n",
    "    3. CURRENT STATUS: What's resolved vs ongoing\n",
    "    4. TRAJECTORY: Getting better or worse? (compare dates)\n",
    "    5. RISK RATING: HIGH/MEDIUM/LOW with specific justification\n",
    "    6. PARTNERSHIP RECOMMENDATION: Your decision with conditions\n",
    "    \n",
    "    Be specific about amounts, dates, and authorities involved.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Prepare evidence for analysis\n",
    "    evidence_prompt = f\"\"\"\n",
    "        Evidence for {entity_name}:\n",
    "\n",
    "        CONFIRMED FINES:\n",
    "        {json.dumps(claims_by_type.get('fine', []), indent=2)}\n",
    "\n",
    "        ACTIVE INVESTIGATIONS:\n",
    "        {json.dumps(claims_by_type.get('investigation', []), indent=2)}\n",
    "\n",
    "        ALLEGATIONS:\n",
    "        {json.dumps(claims_by_type.get('allegation', []), indent=2)}\n",
    "\n",
    "        DISMISSED CASES/CLEARANCES:\n",
    "        {json.dumps(claims_by_type.get('clearance', []), indent=2)}\n",
    "\n",
    "        SETTLEMENT EFFORTS:\n",
    "        {json.dumps(claims_by_type.get('settlement', []), indent=2)}\n",
    "     \n",
    "    \n",
    "    Based on this evidence:\n",
    "    1. Calculate the total fines paid\n",
    "    2. Identify the most serious issue (hint: look for large transaction amounts)\n",
    "    3. Determine if this is historical or ongoing risk\n",
    "    4. Make your partnership recommendation with clear reasoning\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Request LLM Response \")\n",
    "    print(\"Example of evidence_prompt: \", evidence_prompt )\n",
    "    messages = [system_message, HumanMessage(content=evidence_prompt)]\n",
    "    analysis_response = llm_summarization.invoke(messages)\n",
    "    \n",
    "    print(\"Risk assessment completed\")\n",
    "    return {\"messages\": [analysis_response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b749d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add node to the workflow \n",
    "\n",
    "\n",
    "# assemble agent\n",
    "builder = StateGraph(UnifiedResearchState)\n",
    "\n",
    "# content extraction\n",
    "builder.add_node( \"llm\", llm_node)\n",
    "builder.add_node( \"tool_node_chunk_selection_exec\", tool_node_chunk_selection_exec)\n",
    "builder.add_node( \"extract_content\", extract_content)\n",
    "builder.add_node( \"generate_risk_assessment\", generate_risk_assessment)\n",
    "builder.add_node( \"filter_relevant_content\",filter_relevant_content)\n",
    "  # the node DOES wait for all incoming edges to complete before\n",
    "\n",
    "# hyde generation\n",
    "builder.add_node( \"create_journalists\", create_journalists )\n",
    "builder.add_node( \"generate_hyde_document\", generate_hyde_document )\n",
    "\n",
    "# estimate score \n",
    "builder.add_node( \"assign_score_vs_hyde\" ,  assign_score_vs_hyde )\n",
    "\n",
    "# evidence assessment \n",
    "builder.add_node( \"extract_evidence_claims\", extract_evidence_claims)\n",
    "\n",
    "# logic \n",
    "\n",
    "# content extraction\n",
    "builder.add_edge(START, \"llm\")\n",
    "builder.add_edge( \"llm\", \"tool_node_chunk_selection_exec\")\n",
    "builder.add_edge( \"tool_node_chunk_selection_exec\", \"extract_content\")\n",
    "builder.add_edge( \"extract_content\", \"assign_score_vs_hyde\")  # Then end\n",
    "\n",
    "# article writer\n",
    "builder.add_edge( \"llm\" , \"create_journalists\")\n",
    "builder.add_edge( \"create_journalists\" , \"generate_hyde_document\")\n",
    "builder.add_edge( \"generate_hyde_document\" , \"assign_score_vs_hyde\" )\n",
    "builder.add_edge( \"assign_score_vs_hyde\" , \"filter_relevant_content\" )\n",
    "builder.add_edge( \"filter_relevant_content\" , \"extract_evidence_claims\" )\n",
    "builder.add_edge( \"extract_evidence_claims\" , \"generate_risk_assessment\" )\n",
    "builder.add_edge(\"generate_risk_assessment\", END)  # Then end\n",
    "\n",
    "# assemble agent\n",
    "graph = builder.compile( )\n",
    "\n",
    "mermaid_code = graph.get_graph(xray=True).draw_mermaid()\n",
    "render_mermaid_graph(mermaid_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "# Moldretail\n",
    "# Lukoil  Wachovia Bank     HSBC    Dankse Bank\n",
    "# Finpar Invest\n",
    "# trans oil\n",
    "# danube logistics\n",
    "\n",
    "# try different search algoriths\n",
    "# LLM as local \n",
    "# juridical cases in pdf , justice md\n",
    "\n",
    "initial_message = [ HumanMessage(content = \"Dankse Bank\" )  ]\n",
    "results = graph.invoke(   \n",
    "               {\n",
    "                    \"messages\" : initial_message,\n",
    "                    \"max_journalists\": 6,\n",
    "                    \"journalists\": [],  # Will be populated by create_journalists node\n",
    "                    \"hyde_list\": []     # Will be populated by generate_hyde_document node\n",
    "                    \n",
    "                } )\n",
    "results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_res = results.get(\"messages\", None)\n",
    "print( summary_res[-1].content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abc83c",
   "metadata": {},
   "source": [
    "#### Select components to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get('evidence_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = results.get(\"search_results\")[0].model_dump(\n",
    "    exclude={\"raw_content\", \"hyde_score\"}\n",
    ")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1b15a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
