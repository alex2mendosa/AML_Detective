{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed695ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/threading.html\n",
    "\n",
    "# threading are usefull for IO tasks such as file operations or making network requuests when much of a time is \n",
    "# related to waiting for external resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42c8e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MICB_Projects\\8_aml_detective\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# By chat model we mean LLM model which operates with chats \n",
    "import os \n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI \n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal \n",
    "\n",
    "## check performance in agent workflow \n",
    "from typing import TypedDict, Annotated, List, Dict, Optional\n",
    "from langchain_core.messages import BaseMessage, AnyMessage, ToolMessage,HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import add_messages , START, END , StateGraph\n",
    "from IPython.display import Image, display \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# prepare model for embeddings  \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "import numpy.typing as npt\n",
    "\n",
    "from tavily import TavilyClient \n",
    "\n",
    "import subprocess\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "## OpenAI configuration\n",
    "api_key_var = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#print(\"OpenAI API: \" , api_key_var)\n",
    "\n",
    "\n",
    "# keys for tavily \n",
    "tavily_api = os.environ.get(\"TAVILY_API_KEY\")\n",
    "#print(\"Tavily API: \" , tavily_api)\n",
    "\n",
    "# api for goole\n",
    "api_google = os.environ.get('GOOGLE_API_KEY')\n",
    "#print(\"Google API: \" , api_google)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d173dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate summary of extracted content\n",
    "\n",
    "## OpenAI configuration\n",
    "api_key_var = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#print(\"OpenAI API: \" , api_key_var)\n",
    "\n",
    "\n",
    "llm_dummy = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  \n",
    "    temperature=0.2,     \n",
    "    max_tokens=500,      \n",
    "    top_p=0.95,          \n",
    "    timeout=15,         \n",
    ")\n",
    "\n",
    "dummy_statements = [\n",
    "    \"Our compliance dashboard aggregates alerts from transaction monitoring, sanctions screening, and adverse media feeds. Analysts triage cases using severity, age, and source credibility. Weekly reviews track false positives, throughput, and SLA adherence. Quarterly audits compare model outcomes with policy thresholds to justify tuning and ensure regulatory alignment.\",\n",
    "    \"The research assistant ingests URLs in batches, cleans boilerplate, and extracts salient facts. Summaries capture entities, dates, amounts, and jurisdictions while discarding opinion. A consolidation step merges duplicates across outlets. Final outputs include provenance links, confidence notes, and a timeline suitable for auditors and investigative follow-ups.\",\n",
    "    \"For parallel processing, a bounded thread pool dispatches fetch and summarize jobs while honoring rate limits. Retries use exponential backoff with jitter. Structured errors capture domain, stage, attempt, and message. Results are reassembled in input order, enabling deterministic runs, easy diffing, and quick root-cause analysis when failures occur.\",\n",
    "    \"Risk evaluation weighs confirmed fines, ongoing investigations, and settlements more heavily than allegations. Jurisdictional diversity and recency increase materiality. Where facts conflict, the system elevates disagreement and tags items for manual review. Strict schema validation prevents malformed claims from propagating, improving trust in downstream scoring and reporting components.\",\n",
    "    \"Token accounting records input and output usage per stage, per URL, and per batch. Counters roll up to daily totals by model family to forecast cost and throughput. Anomalies trigger alerts when variance exceeds thresholds. These metrics inform capacity planning, budget approvals, and pragmatic choices about model selection.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def generate_url_summary(raw_content: str, llm, results:dict, idx:int):\n",
    "    \n",
    "    prompt = f\"Extract main idea from text {raw_content}\"\n",
    "           \n",
    "    response = llm.invoke(prompt)\n",
    "    summary = response.content.strip()\n",
    "    results[idx] = summary\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596f1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 'The main idea is that token accounting tracks usage and aggregates data to forecast costs and throughput, detect anomalies, and support capacity planning, budgeting, and model selection decisions.', 0: 'The compliance dashboard consolidates alerts from various monitoring sources, enabling analysts to prioritize cases based on severity and credibility, while regular reviews and audits ensure accuracy, efficiency, and regulatory compliance.', 2: 'The main idea is that a bounded thread pool manages parallel fetch and summarize tasks within rate limits, using exponential backoff with jitter for retries, structured error tracking, and reassembling results in input order to ensure deterministic execution and facilitate debugging.', 1: 'The research assistant processes batches of URLs to clean and extract key factual information, summarizes important details while excluding opinions, consolidates duplicates, and produces final outputs with source links, confidence levels, and timelines for auditing and investigations.', 3: 'The main idea is that the risk evaluation process prioritizes confirmed fines, ongoing investigations, and settlements over mere allegations, considers jurisdictional diversity and recency to assess materiality, flags conflicting information for manual review, and uses strict schema validation to ensure data quality and reliability in scoring and reporting.'}\n"
     ]
    }
   ],
   "source": [
    "import threading \n",
    "import time \n",
    "\n",
    "# Dictionary to collect results (thread-safe for simple assignments)\n",
    "results = {}\n",
    "threads = []\n",
    "\n",
    "# create and start threads  \n",
    "for idx, text in enumerate(dummy_statements):\n",
    "    t = threading.Thread(target = generate_url_summary, args = (text, llm_dummy, results , idx) )\n",
    "    threads.append(t)\n",
    "# We are creating threads and we tell what to do\n",
    "\n",
    "# Start each thread\n",
    "for t in threads:\n",
    "    t.start() # RuntimeError: threads can only be started once\n",
    "\n",
    "# wait thread to finish\n",
    "for t in threads:\n",
    "    t.join()    \n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd37bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threading.Thread creates worker for my function, but the process is not initiated yet\n",
    "   # we use it to assing work to each thread\n",
    "\n",
    "# t.start() no we need to begin the execution \n",
    "# t.join() waut thread tofinish before the execution \n",
    "\n",
    "# Now thread cant return values directly , we use return to \n",
    "  # have shared dictionary \n",
    "  # queue \n",
    "# Good for: I/O-bound tasks (API calls, file reading, web scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33bedd",
   "metadata": {},
   "source": [
    "#### Prepare snippet for main code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c87d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate summary of extracted content\n",
    "\n",
    "## dummy data \n",
    "\n",
    "url_to_content = {\n",
    "    \"https://news.example.com/article/alpha\": \"Alpha Corp announced a strategic partnership to expand its AI-driven risk analytics across European markets. The deal includes data-sharing agreements, a joint research roadmap, and a pilot program with three banks scheduled for Q1 next year.\",\n",
    "    \"https://blog.example.org/posts/data-pipelines\": \"A step-by-step guide to building reliable data pipelines with Python and PostgreSQL. Covers schema design, idempotent ETL jobs, observability basics, and how to version transformations to ensure reproducibility.\",\n",
    "    \"https://docs.example.dev/quickstart\": \"Quickstart for the Example SDK: install the package, authenticate with an API key, initialize the client, and call the /summarize endpoint. Includes code snippets, rate limit notes, and links to pagination and error-handling sections.\",\n",
    "    \"https://portal.example.net/status/2025-11-07\": \"Incident report: elevated latency in EU-West between 14:05â€“14:42 EET due to a misconfigured cache layer. Mitigated by rolling back the config and purging stale entries. Postmortem to follow with action items on alert thresholds and canary tests.\"\n",
    "}\n",
    "\n",
    "thread_outcomes = {}\n",
    "\n",
    "def generate_url_summary(raw_content: str, llm, entity_name: str, thread_outcomes: dict, url: str):\n",
    "\n",
    "    print(f\"Thread for {url} starting...\")\n",
    "\n",
    "    # Input validation\n",
    "    if not raw_content or not raw_content.strip():\n",
    "        thread_outcomes[url] = \"Error: No content provided for analysis\"\n",
    "        return\n",
    "      \n",
    "    # Truncate content if too long (prevent token limit issues)\n",
    "    max_content_length = 8000  # Adjust based on your model's limits\n",
    "    if len(raw_content) > max_content_length:\n",
    "        raw_content = raw_content[:max_content_length] + \"...[truncated]\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            You are analyzing an article about {entity_name} company for anti-money laundering due diligence.\n",
    "\n",
    "            Content to analyze:\n",
    "            {raw_content}\n",
    "\n",
    "            Return answer if the information contains an mention of money laudering , \n",
    "            if none, respond that data in article is irrelevant\n",
    "\n",
    "            \"\"\"\n",
    "           \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Validate response\n",
    "        if not response or not hasattr(response, 'content'):\n",
    "            thread_outcomes[url] = \"Error: Invalid response from language model\"\n",
    "            return\n",
    "        \n",
    "        summary = response.content.strip()\n",
    "        thread_outcomes[url] = summary\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        # More specific error handling\n",
    "        error_msg = f\"Error generating reputation summary: {str(e)}\"\n",
    "        print(error_msg)  # For debugging\n",
    "        thread_outcomes[url] = \"Error: Could not generate reputation summary due to processing issues\"\n",
    "        return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f779ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread for https://news.example.com/article/alpha starting...\n",
      "Thread for https://blog.example.org/posts/data-pipelines starting...\n",
      "Thread for https://docs.example.dev/quickstart starting...\n",
      "Thread for https://portal.example.net/status/2025-11-07 starting...\n",
      "{'https://blog.example.org/posts/data-pipelines': 'The data in the article is irrelevant.', 'https://docs.example.dev/quickstart': 'Data in article is irrelevant.', 'https://news.example.com/article/alpha': 'The data in the article is irrelevant.', 'https://portal.example.net/status/2025-11-07': 'The data in the article is irrelevant to money laundering.'}\n"
     ]
    }
   ],
   "source": [
    "# now trreading\n",
    "threads = []\n",
    "entity_name = \"Example Corp\" \n",
    "\n",
    "for url, raw_content in url_to_content.items():\n",
    "    t = threading.Thread( \n",
    "        target = generate_url_summary ,\n",
    "        args = ( raw_content, llm_dummy, entity_name, thread_outcomes, url )\n",
    "     )\n",
    "    threads.append(t)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start() \n",
    "\n",
    "for thread in threads:\n",
    "    t.join()\n",
    "\n",
    "print(thread_outcomes)            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
