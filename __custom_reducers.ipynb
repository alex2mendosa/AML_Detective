{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fb793c",
   "metadata": {},
   "source": [
    "We need custom Reducers which append list made out of classes , based on values of attributes in this classes\n",
    "\n",
    "\n",
    "1 We need to change architecrure of flow. HyDe genareation must be implemented as subgraph\n",
    "It will share with main hyde scores\n",
    "But what if we want to keep and check quality of hyde documents(DOne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e93ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MICB_Projects\\8_aml_detective\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# By chat model we mean LLM model which operates with chats \n",
    "import os \n",
    "import time\n",
    "import re\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from openai import OpenAI\n",
    "\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from langchain_openai import ChatOpenAI \n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal , Optional\n",
    "\n",
    "## check performance in agent workflow \n",
    "from typing import TypedDict, Annotated, List, Dict, Optional, Set\n",
    "from langchain_core.messages import BaseMessage, AnyMessage, ToolMessage,HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import add_messages , START, END , StateGraph\n",
    "from IPython.display import Image, display \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# prepare model for embeddings  \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "import numpy.typing as npt\n",
    "\n",
    "from tavily import TavilyClient \n",
    "\n",
    "import subprocess\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fce9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIVault():\n",
    "    def __init__(self):  # Fixed: single underscore\n",
    "        self._keys = {}\n",
    "\n",
    "    def add_key(self, key_name: str, key_value: str):\n",
    "        \"\"\"Add API key to the vault\"\"\"\n",
    "        if not key_name:\n",
    "            print(\"Key name is empty\")\n",
    "        elif not key_value:  # Added check for value\n",
    "            print(f\"Key value is empty for '{key_name}'\")\n",
    "        else:   \n",
    "            self._keys[key_name] = key_value\n",
    "        \n",
    "    def get_key(self, key_name: str):\n",
    "        \"\"\"Get an API key from the vault\"\"\"\n",
    "        if key_name not in self._keys:\n",
    "            print(\"Key is not present in Vault\")\n",
    "            return None  # Added return\n",
    "        else:\n",
    "            return self._keys[key_name]  # Fixed: use _keys dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43de354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value is empty for 'dummy'\n"
     ]
    }
   ],
   "source": [
    "## populate classes\n",
    "key_vault = APIVault()\n",
    "key_vault.add_key( \"openai_key\" , os.environ.get(\"OPENAI_API_KEY\")) # personal api key\n",
    "key_vault.add_key(\"tavily_key\" ,os.environ.get(\"TAVILY_API_KEY\")) # for tavily search and extract\n",
    "key_vault.add_key(\"google_key\" ,os.environ.get(\"GOOGLE_API_KEY\")) # gor google search engine\n",
    "key_vault.add_key(\"openai_key_azure_41\" , os.environ.get(\"OPENAI_API_KEY_AZURE_41\")) # for azure project for gpt 4.1\n",
    "key_vault.add_key(\"dummy\" , os.environ.get(\"dummy\")) # to test error mesage\n",
    "key_vault.add_key(\"perplexity_key\" , os.environ.get(\"PERPLEXITY_API_KEY\")) # to test error mesage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b34e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\MICB_Projects\\8_aml_detective\\prompts\\prompts_search_tools.txt\n",
      "d:\\MICB_Projects\\8_aml_detective\\prompts\\prompts_structured_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Support for Google API \n",
    "# define search engine\n",
    "SEARCH_ENGINE_ID = '26dda816634bd4044'\n",
    "\n",
    "## Azure service \n",
    "endpoint = 'https://datam-mhtcc5x5-westeurope.cognitiveservices.azure.com/openai/v1'\n",
    "# The SDK automatically appends /chat/completions and other paths\n",
    "\n",
    "# azure https://datam-mhtcc5x5-westeurope.cognitiveservices.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\n",
    "# This is the direct REST API endpoint, Used for raw HTTP requests (curl, requests library, etc.)\n",
    "\n",
    "## Path to prompts\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROMPTS_TOOLS = os.path.join(CURRENT_DIR , \"prompts\\prompts_search_tools.txt\")\n",
    "PROMPTS_STR_OUTPUT = os.path.join(CURRENT_DIR , \"prompts\\prompts_structured_output.txt\")\n",
    "\n",
    "print(PROMPTS_TOOLS)\n",
    "print(PROMPTS_STR_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245000b5",
   "metadata": {},
   "source": [
    "#### Test OpenAI SDK - Azure Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481b07e",
   "metadata": {},
   "source": [
    "#### LLM models api definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287bb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM to select tool, populate class with restricted output, [ tools_name_1, ... ,  exit scenario  ]\n",
    "# short deterministic outputs\n",
    "llm_tool_selection = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  \n",
    "    temperature=0.2,     \n",
    "    max_tokens=50, # Max number of tokens to generate. |\n",
    "    top_p=0.95,          \n",
    "    timeout=30,  \n",
    "    max_retries = 2,\n",
    "    api_key=key_vault.get_key(\"openai_key\")       \n",
    ") # OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY. |\n",
    "\n",
    "\n",
    "# Model to create payload for search tools, its pivotal to create exactly 5 queries, add retry logic inside node\n",
    "llm_search_tool_payload = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  \n",
    "    temperature=0.2,     \n",
    "    max_tokens=500, # 5 outputs per 5 languages     \n",
    "    top_p=0.95,          \n",
    "    timeout=30,  \n",
    "    max_retries = 2,\n",
    "    api_key=key_vault.get_key(\"openai_key\")             \n",
    ")\n",
    "\n",
    "## we also need to generate names variations in different languages\n",
    "## output become input to geenerate regular expression , google search allows search modifiers but other search tools may not\n",
    "llm_names_variation = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",   # note the hyphen\n",
    "    temperature=0.0,        # pure extraction/translation\n",
    "    max_tokens=50,          # we only need one short line\n",
    "    timeout=30,  \n",
    "    max_retries = 2,\n",
    "    api_key=key_vault.get_key(\"openai_key\")    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Translation and key terms extraction from short input\n",
    "llm_translation_or_terms = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",  \n",
    "    temperature=0.1,      \n",
    "    max_tokens=500,       \n",
    "    timeout=30,           \n",
    "    max_retries=2,         \n",
    "    api_key=key_vault.get_key(\"openai_key\")                         \n",
    ")\n",
    "\n",
    "\n",
    "# # generate persona\n",
    "llm_expert_generation = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",     # BETTER: Cheaper, perfectly capable for structured output\n",
    "    temperature=0.2,         # BEST: Near-deterministic for consistent structure\n",
    "    max_tokens=1000,         \n",
    "    timeout=60,             \n",
    "    max_retries=3,           # ADEQUATE: Structured output usually works first try\n",
    "    api_key=key_vault.get_key(\"openai_key\")   \n",
    ")\n",
    "# LengthFinishReasonError: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=200, prompt_tokens=426, total_\n",
    "# APITimeoutError: Request timed out.\n",
    "# 4.0 mini replaced with 4.1 as it offers larger context allowing to simulate more experts\n",
    "\n",
    "\n",
    "llm_hyde_generation = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",     # OPTIMAL: Cost-effective for creative writing\n",
    "    temperature=0.5,         # BEST: Creative variation for different journalist styles\n",
    "    max_tokens=500,          \n",
    "    timeout=60,            \n",
    "    max_retries=3,\n",
    "    api_key=key_vault.get_key(\"openai_key\")             \n",
    ")\n",
    "\n",
    "\n",
    "########################################### AZURE\n",
    "# Duw to error below , before we get better result, try personal data.\n",
    "\n",
    "# Error generating reputation summary: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your \n",
    "# prompt and retry. To learn more about our content filtering policies please read our documentation:\n",
    "# https://learn.microsoft.com/en-us/answers/questions/1297066/i-keep-getting-this-error-exception-in-chat-messag\n",
    "\n",
    "#Error generating reputation summary: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': \n",
    "#'Your requests to gpt-4.1 for gpt-4.1 in West Europe have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under OpenAI Language Model Instance API. Please retry after 60 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
    "\n",
    "# Summary of content extracted from URL with tavily, applieed per each link\n",
    "llm_url_content_summary = ChatOpenAI(\n",
    "    api_key=key_vault.get_key(\"openai_key\"),    \n",
    "    #base_url = f\"{endpoint}\" , \n",
    "    #api_key = key_vault.get_key(\"openai_key_azure_41\"), \n",
    "    model=\"gpt-4.1\",  # BETTER CHOICE: Your original \"gpt-4o\" was budget-killer\n",
    "                          # URL content analysis is mostly extraction/summarization\n",
    "    \n",
    "    temperature=0.2,      # BEST: Factual analysis needs consistency, not creativity\n",
    "    max_tokens=2000,     \n",
    "    top_p=0.95,         \n",
    "    timeout=60,\n",
    "    max_retries=3           \n",
    ")\n",
    "\n",
    "\n",
    "# Applied for extract_evidence_claims, consolidates results from Multiple summaries\n",
    "llm_agg_summaries = ChatOpenAI(\n",
    "    #api_key=key_vault.get_key(\"openai_key\"), \n",
    "    base_url = f\"{endpoint}\" , \n",
    "    api_key = key_vault.get_key(\"openai_key_azure_41\") , \n",
    "\n",
    "    model=\"gpt-4.1\", \n",
    "    temperature=0.1,      \n",
    "    max_tokens=2000,     \n",
    "    top_p=0.95,          \n",
    "    timeout=60,          \n",
    "    max_retries=3        \n",
    ")\n",
    "\n",
    "\n",
    "# Model uses multiple summaries to make conclusion about possible risk \n",
    "llm_evaluation = ChatOpenAI( # compare to 4.o should have better instrusction following\n",
    "    #api_key=key_vault.get_key(\"openai_key\"), \n",
    "    base_url = f\"{endpoint}\" , \n",
    "    api_key = key_vault.get_key(\"openai_key_azure_41\") , \n",
    "    model=\"gpt-4.1\", # BadRequestError: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: gpt-4.o', 'details': 'Unknown model: gpt-4.o'}}      \n",
    "    temperature=0.2,                   \n",
    "    max_tokens=3000,     \n",
    "    top_p=0.95, # not applicable for 5 openai family        \n",
    "    timeout=60,          \n",
    "    max_retries=3        \n",
    ")\n",
    "\n",
    "# BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'top_p' is not supported with this model.\", \n",
    "# 'type': 'invalid_request_error', 'param': 'top_p', 'code': 'unsupported_parameter'}}\n",
    "\n",
    "## test gpt 5, takes signifficnt time, requires larger timeout\n",
    "# result = llm_evaluation.invoke(\"Generate recommendations on how to prepare banana bread\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49492845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.com: hyde_score=None, summary=''\n",
      "[LinkCollection(displayLink='example.com', link='https://example.com/article1', claim_type='other', date_published=None, search_engine='engine_1', severity_level='', summary='', hyde_score=None, raw_content='')]\n"
     ]
    }
   ],
   "source": [
    "# Data and Meta for extracted content, per single link\n",
    "\n",
    "# First model selects tool and return it, then \n",
    "# we pass specific System message to LLM with Tools.\n",
    "# So essentially:\n",
    "# Get Name of Tool -> Select System Message -> Pass system message to LLM with Tools\n",
    "\n",
    "ToolsPool = Literal[\"google_search\",\"perplexity_search\",\"tavily_search\",\"exit_scenario\"]\n",
    "\n",
    "class Tool_Selected(BaseModel):\n",
    "       search_engine:ToolsPool = Field(default=\"\" , description=\"Name of tool used to suggest url link\") # taken from tool call tool.name\n",
    "\n",
    "class LinkCollection(BaseModel): # organise for better visual inspection \n",
    "       \n",
    "       displayLink: str = Field(description=\"The display URL shown in search results (usually domain name)\") # passeed from search tool\n",
    "       link: str = Field(description=\"The full URL of the search result\") # passeed from search tool\n",
    "\n",
    "       claim_type: str = Field(default=\"other\", description=\"Each article must fit specific only 1 claim type\") # from ContentSummary\n",
    "       date_published: Optional[str] = Field( default=None, description=\"Publication date extracted from content (format: YYYY-MM-DD, or 'Unknown' if not found)\" ) # from ContentSummary\n",
    "       search_engine: str = Field(default=\"\" , description=\"Name of tool used to suggest url link\") # taken from tool call tool.name\n",
    "       severity_level: str = Field(default=\"\", description=\"Each article must fit specific only 1 severity level\") # from ContentSummary\n",
    "       summary: str = Field(default=\"\", description=\"Summary of content extracted from URL\") # from ContentSummary\n",
    "\n",
    "       hyde_score: Optional[float] = Field(default=None, description=\"Max value of similarity between content of web page and HyDe articles\") # from similarity score\n",
    "                                           \n",
    "       raw_content: str = Field(default=\"\", description=\"Content extracted from URL\") # passed from content parser                                    \n",
    "                                           \n",
    " \n",
    " # how to update LinkCollection\n",
    "dummy_list = [\n",
    "    LinkCollection(displayLink=\"example.com\", link=\"https://example.com/article1\")\n",
    "]\n",
    "\n",
    "for item in dummy_list:\n",
    "    print(f\"{item.displayLink}: hyde_score={item.hyde_score}, summary='{item.summary}'\")\n",
    "\n",
    "# dummy_list[0].dummy_attribute =\"engine_1\" # ValueError: \"LinkCollection\" object has no field \"dummy_attribute\"    \n",
    "dummy_list[0].search_engine =\"engine_1\" # so its not immutbale we can modify in place \n",
    "\n",
    "print(dummy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7726f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add \n",
    "\n",
    "class UnifiedResearchState(TypedDict):\n",
    "    search_results_kw_filter: Annotated[List[LinkCollection] , operator.add] # applied during content extraction, filter via regexp\n",
    "\n",
    "\n",
    "class LinkCollection(BaseModel): # organise for better visual inspection \n",
    "       link: str = Field(description=\"The full URL of the search result\") # passeed from search tool\n",
    "       hyde_score: Optional[float] = Field(default=None, description=\"Max value of similarity between content of web page and HyDe articles\") # from similarity score                          \n",
    "                             \n",
    "                                           \n",
    " \n",
    "l1 = [ LinkCollection(link = \"url_1\", hyde_score=0.1) , LinkCollection(link = \"url_2\", hyde_score=None), LinkCollection(link = \"url_3\", hyde_score=0.6) ]\n",
    "l2 = [ LinkCollection(link = \"url_2\", hyde_score=0.5) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reducer(existing:list , new:list) -> list: \n",
    "    links  = [item.link for item in new]\n",
    "    scores = [item.hyde_score for item in new]\n",
    "    url_hyde_map = {link: score for link, score in zip(links, scores)}\n",
    "\n",
    "    ## now replace values in the original \n",
    "    for item in existing: \n",
    "        if item.link in url_hyde_map: \n",
    "           item.hyde_score =  url_hyde_map.get(item.link)\n",
    "\n",
    "    return existing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc36f477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinkCollection(link='url_1', hyde_score=0.1),\n",
       " LinkCollection(link='url_2', hyde_score=0.5),\n",
       " LinkCollection(link='url_3', hyde_score=0.6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_reducer(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1b15a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
